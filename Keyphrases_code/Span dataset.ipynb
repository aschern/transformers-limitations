{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def sents_token_bounds(text):\n",
    "    sents_starts = []\n",
    "    for start, end in PunktSentenceTokenizer().span_tokenize(text):\n",
    "        sents_starts.append(start)\n",
    "    sents_starts.append(100000)\n",
    "    return np.array(sents_starts)\n",
    "\n",
    "\n",
    "def clear(text):\n",
    "    return unidecode(text.strip().replace('\\t', ' ').replace('\\n', ' '))\n",
    "\n",
    "\n",
    "def get_context(article, span_start, span_end):\n",
    "    bounds = sents_token_bounds(article)\n",
    "    context_start = bounds[np.where(bounds <= span_start)[0][-1]]\n",
    "    context_end = bounds[np.where(bounds >= span_end)[0][0]]\n",
    "    return clear(article[context_start:context_end])\n",
    "\n",
    "\n",
    "def get_markup(part):\n",
    "    span_data = []\n",
    "    ids = list(set([file[:-4] for file in os.listdir('alt_data/{}'.format(part))]))\n",
    "    for file_id in ids:\n",
    "        data = pd.read_csv(os.path.join('alt_data/{}'.format(part), file_id + '.ann'), sep='\\t', header=None)\n",
    "        with open(os.path.join('alt_data/{}'.format(part), file_id + '.txt'), 'r') as f:\n",
    "            text = f.read()\n",
    "        for i, row in data.iterrows():\n",
    "            try:\n",
    "                label, span_start, span_end = row[1].split()\n",
    "                span_start, span_end = int(span_start), int(span_end)\n",
    "            except:\n",
    "                continue\n",
    "            assert text[span_start:span_end] == row[2]\n",
    "            context = get_context(text, span_start, span_end)\n",
    "            span_data.append((row[2], context, label, span_start, span_end, file_id))\n",
    "    df = pd.DataFrame(span_data)\n",
    "    if part == 'train':\n",
    "        df = df.sample(frac=1)\n",
    "    df.to_csv('alt_data/{}.tsv'.format(part), header=None, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in ['train', 'dev', 'test']:\n",
    "    get_markup(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def token_label_from_spans(pos, spans):\n",
    "    for el in spans:\n",
    "        if el[0] <= int(pos) < el[1]:\n",
    "            return \"PROP\"\n",
    "    return 'O'\n",
    "\n",
    "\n",
    "def create_BIO_labeled(part, nlp, max_length=32):\n",
    "    prev_label = 'O'\n",
    "    cur_length = 0\n",
    "    with open('alt_data/{}_bio.tsv'.format(part), 'w') as f:\n",
    "        ids = list(set([file[:-4] for file in os.listdir('alt_data/{}'.format(part))]))\n",
    "        for file_id in ids:\n",
    "            spans = []\n",
    "            data = pd.read_csv(os.path.join('alt_data/{}'.format(part), file_id + '.ann'), sep='\\t', header=None)\n",
    "            with open(os.path.join('alt_data/{}'.format(part), file_id + '.txt'), 'r') as fin:\n",
    "                text = fin.read()\n",
    "            \n",
    "            for i, row in data.iterrows():\n",
    "                try:\n",
    "                    label, span_start, span_end = row[1].split()\n",
    "                    span_start, span_end = int(span_start), int(span_end)\n",
    "                except:\n",
    "                    continue\n",
    "                assert text[span_start:span_end] == row[2]\n",
    "                spans.append((span_start, span_end))\n",
    "            \n",
    "            tokens = [(token.idx, token.text) for token in nlp(text)]\n",
    "            idx = np.array(tokens)[:,0]\n",
    "            tokens = np.array(tokens)[:,1]\n",
    "            prev_tok = '\\n'\n",
    "            \n",
    "            for i in range(len(tokens)):\n",
    "                tok = tokens[i].replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "                if len(tok) != 0 and repr(tok) != repr('\\ufeff') and repr(tok) != repr('\\u200f'):\n",
    "                    tok = tokens[i].strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "                    label =  token_label_from_spans(idx[i], spans)\n",
    "                    if label != 'O':\n",
    "                        if prev_label != 'O':\n",
    "                            label = 'I-' + 'KEY'\n",
    "                        else:\n",
    "                            label = 'B-' + 'KEY'\n",
    "                    if cur_length + 1 + 2 > max_length:\n",
    "                        f.write('\\n')\n",
    "                        cur_length = 0\n",
    "                    f.write(tok + '\\t' + label + '\\n')\n",
    "                    cur_length += 1\n",
    "                    prev_label = label\n",
    "                    prev_tok = tok\n",
    "                else:\n",
    "                    if prev_tok != '\\n':\n",
    "                        # f.write('\\n')\n",
    "                        prev_tok = '\\n'\n",
    "                    prev_label = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in ['train', 'dev', 'test']:\n",
    "    create_BIO_labeled(part, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
